# -*- coding: utf-8 -*-
"""Ollama with LangChain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/127dIgwc2PsHh7egOeRKTKuaECfcrnPOm
"""

!sudo apt-get install -y pciutils
!curl -fsSL https://ollama.com/install.sh | sh # download ollama api
from IPython.display import clear_output

# Create a Python script to start the Ollama API server in a separate thread

import os
import threading
import subprocess
import requests
import json

def ollama():
    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'
    os.environ['OLLAMA_ORIGINS'] = '*'
    subprocess.Popen(["ollama", "serve"])

ollama_thread = threading.Thread(target=ollama)
ollama_thread.start()

from IPython.display import clear_output
!ollama pull llama3.1:8b
clear_output()

!pip install langchain-core langchain-community

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StroutputParser
from langchain_community.llms import Ollama
import streamlit as st

st.title("Mohamed's Chat Bot")
input_txt = st.text_input("Please enter your queries here...")


prompt = ChatPromptTemplate.from_messages(
[("system", "you are a helpful AI assistant. Your name is Mohamed's Assistant"),
("user", "user query: {query}")
])

llm = ollama (model="llama3")
output_parser = StroutputParser()
chain = prompt | llm | output_parser

if input_txt:
st.write(chain.invoke({"query": input_txt}))